{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn LOGGED IN Successfully......../Extracting the postings\n",
      "First (in loop): https://www.linkedin.com/jobs/search/?keywords=Data+Analyst&location=Texas&sort=date&start=0&f_E=1%2C2%2C3&f_TPR=r2592000\n",
      "First (in loop): https://www.linkedin.com/jobs/search/?keywords=Data+Analyst&location=California&sort=date&start=0&f_E=1%2C2%2C3&f_TPR=r2592000\n",
      "First (in loop): https://www.linkedin.com/jobs/search/?keywords=Analyst&location=Texas&sort=date&start=0&f_E=1%2C2%2C3&f_TPR=r2592000\n",
      "First (in loop): https://www.linkedin.com/jobs/search/?keywords=Analyst&location=California&sort=date&start=0&f_E=1%2C2%2C3&f_TPR=r2592000\n",
      "First (in loop): https://www.linkedin.com/jobs/search/?keywords=Business+Analyst&location=Texas&sort=date&start=0&f_E=1%2C2%2C3&f_TPR=r2592000\n",
      "First (in loop): https://www.linkedin.com/jobs/search/?keywords=Business+Analyst&location=California&sort=date&start=0&f_E=1%2C2%2C3&f_TPR=r2592000\n",
      "First (in loop): https://www.linkedin.com/jobs/search/?keywords=Medicaid+analyst&location=Texas&sort=date&start=0&f_E=1%2C2%2C3&f_TPR=r2592000\n",
      "First (in loop): https://www.linkedin.com/jobs/search/?keywords=Medicaid+analyst&location=California&sort=date&start=0&f_E=1%2C2%2C3&f_TPR=r2592000\n",
      "CSV file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import urllib.parse\n",
    "import requests\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\"\"\"\n",
    "This script scrapes job postings from LinkedIn for specific job titles and locations.\n",
    "\n",
    "It uses Selenium to automate browser actions and BeautifulSoup to parse HTML content.\n",
    "The extracted data is then saved to a CSV file.\n",
    "\n",
    "Requirements:\n",
    "    - Python 3.6+\n",
    "    - Selenium\n",
    "    - BeautifulSoup4\n",
    "    - Pandas\n",
    "    - Requests\n",
    "    - webdriver-manager (install using: pip install webdriver-manager)\n",
    "\"\"\"\n",
    "\n",
    "# Define the login credentials\n",
    "username = \"kathisnehithreddy@gmail.com\"\n",
    "password = \"Ksnehith@123\"\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "service = Service(executable_path=r\"C:\\Users\\snehi\\Desktop\\Linkedin_extract\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "#driver = webdriver.Chrome(executable_path=r\"C:\\Users\\snehi\\Desktop\\Linkedin_extract\\chromedriver.exe\") # latest 130 chrome driver\n",
    "driver.maximize_window()\n",
    "\n",
    "# Navigate to the LinkedIn login page\n",
    "driver.get(\"https://www.linkedin.com/login\")\n",
    "\n",
    "# Find the input fields and enter the login credentials\n",
    "username_field = driver.find_element(By.ID, \"username\")\n",
    "username_field.send_keys(username)\n",
    "\n",
    "password_field = driver.find_element(By.ID, \"password\")\n",
    "password_field.send_keys(password)\n",
    "\n",
    "# Submit the login form\n",
    "password_field.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the page to load after login\n",
    "time.sleep(5)\n",
    "\n",
    "# Set user agent for requests\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "headers = {'User-Agent': user_agent}\n",
    "\n",
    "print(\"LinkedIn LOGGED IN Successfully......../Extracting the postings\")\n",
    "\n",
    "# Initialize data storage\n",
    "data = []\n",
    "data_col = ['Job_title', 'City', 'State', 'Country', 'Company', 'Post_date', 'Job_link', 'Company_link']\n",
    "\n",
    "# Define job titles and locations to search for\n",
    "job_titles = ['Data Analyst', 'Analyst', 'Business Analyst', \"Medicaid analyst\" ]  # List of job titles to search for\n",
    "locations = ['Texas', 'California']  # List of locations to search in\n",
    "\n",
    "# Main loop for scraping job postings\n",
    "# Iterates through job titles, locations, and pages of search results\n",
    "# Extracts relevant information from each job posting and appends to data list\n",
    "\n",
    "for job_title in job_titles:\n",
    "    for location in locations:\n",
    "        for page in range(1):  # Iterate over multiple pages\n",
    "            getVars = {'keywords': job_title, 'location': location, 'sort': 'date', 'start': str(page * 10), 'f_E': '1,2,3', 'f_TPR': 'r2592000'}\n",
    "            url = 'https://www.linkedin.com/jobs/search/?' + urllib.parse.urlencode(getVars)\n",
    "            print('First (in loop):', url)\n",
    "\n",
    "            response = requests.get(url, headers=headers, verify=False)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            soup\n",
    "            extract_posting = soup.find_all('div', class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n",
    "\n",
    "            for post in extract_posting:\n",
    "                position_title = str(post.find('h3', class_='base-search-card__title').text.strip())\n",
    "                \n",
    "                location_full = str(post.find('span', class_='job-search-card__location').text.strip())\n",
    "                location_parts = location_full.split(', ')\n",
    "                \n",
    "                city = location_parts[0] if len(location_parts) > 0 else None\n",
    "                state = location_parts[1] if len(location_parts) > 1 else None\n",
    "                country = location_parts[2] if len(location_parts) > 2 else None\n",
    "                \n",
    "                position_company = str(post.find('h4', class_=\"base-search-card__subtitle\").text.strip())\n",
    "                position_date = datetime.strptime(post.find('time')['datetime'], '%Y-%m-%d').date()\n",
    "                position_Link = post.find('a', class_='base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')['href']\n",
    "                position_companylink = post.find('a', class_='hidden-nested-link')['href']\n",
    "\n",
    "                data.append([\n",
    "                    position_title, city, state, country, \n",
    "                    position_company, position_date, \n",
    "                    position_Link, position_companylink\n",
    "                ])\n",
    "\n",
    "# Create a Pandas DataFrame from the extracted data\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=data_col)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(\"linkedinJobs_data.csv\", index=False)\n",
    "\n",
    "print(\"CSV file saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
